![thumbnail](thumbnail.png)

# OpenELM, Phi-3, Quantized LLaMA-3

### Links

**YouTube:** https://youtube.com/live/YEm4tuo2HPA

**X:** https://twitter.com/i/broadcasts/1mrxmynMbakxy

**Twitch:** 

**Substack:**

**ResearchHub:**

**TikTok:**

**Reddit:**

### References

OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework
https://arxiv.org/pdf/2404.14619

Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
https://arxiv.org/pdf/2404.14219

How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study
https://arxiv.org/pdf/2404.14047

GQA
https://paperswithcode.com/method/grouped-query-attention

LayerNorm, RMS, Fused Ops
https://static-assets.codecademy.com/Paths/data-science-career-path/MachineLearning/outlier.png
https://tungmphung.com/wp-content/uploads/2021/01/Screenshot-from-2021-01-05-13-07-25-1024x274.png
https://github.com/pytorch/pytorch/issues/72643
https://github.com/tinygrad/tinygrad/issues/1146

SwiGLU
https://paperswithcode.com/method/silu

FlashAttention
https://miro.medium.com/v2/resize:fit:2000/1*i4tDdwgvGtXuTIyJpFUn8A.png

datatypes
https://www.microsoft.com/en-us/research/uploads/prod/2020/12/Brainwave_figure2.jpg

decoder only
https://miro.medium.com/v2/resize:fit:863/0*jKqypwGzmDv7KDUZ.png

LoRA
https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html
https://miro.medium.com/v2/resize:fit:523/1*F7uWJePoMc6Qc1O2WxmQqQ.png
