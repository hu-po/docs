![thumbnail](thumbnail.png)

# Gamba Jamba Cobra

### Links

**YouTube:** https://youtube.com/live/9s-9aSobky8

**X:** https://twitter.com/i/broadcasts/1ZkKzjydlkaKv

**Twitch:**

**Substack:**

**ResearchHub:**

**TikTok:**

**Reddit:**

### References

https://www.ai21.com/blog/announcing-jamba

Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference 
- https://arxiv.org/pdf/2403.14520.pdf
- https://www.emergentmind.com/papers/2403.14520

Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction
- https://arxiv.org/pdf/2403.18795.pdf
- https://www.emergentmind.com/papers/2403.18795

Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models
- https://arxiv.org/pdf/2402.19427.pdf

Mamba: Linear-Time Sequence Modeling with Selective State Spaces
- https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf

The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
https://arxiv.org/pdf/2402.17764.pdf

https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGh_0y3ICNuA6IcnbdnvdA.gif

## Notes

notes

### Blog

notes

### Vertical Video

title
description
hashtags
